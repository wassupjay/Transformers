{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "119d8cb6-2199-4d9e-bf65-f7c947b2a7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[  101, 19081,  2024,  1996,  6429,   102]])\n",
      "Attention Mask: tensor([[1, 1, 1, 1, 1, 1]])\n",
      "Embedding shape: torch.Size([1, 6, 768])\n",
      "Embedding: tensor([[[-0.2035, -0.2107, -0.0598,  ..., -0.0067,  0.6471,  0.1615],\n",
      "         [ 0.7369,  0.1867, -0.0275,  ..., -0.1056,  0.5047, -0.1240],\n",
      "         [-0.1810,  0.1425,  0.1476,  ..., -0.2274,  0.4797,  0.0649],\n",
      "         [-0.2215, -0.4444, -0.1097,  ...,  0.1328,  0.7340, -0.5387],\n",
      "         [-0.1936,  0.0585, -0.2855,  ...,  0.2360,  0.4225, -0.9133],\n",
      "         [ 0.9309, -0.0113, -0.2193,  ...,  0.1450, -0.3424, -0.2938]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "[CLS] transformers are the amazing [SEP]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Your input sentence\n",
    "sentence = \"transformers are the amazing\"\n",
    "\n",
    "# Tokenize and encode the sentence\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Extract input_ids and attention_mask\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "print(\"Input IDs:\", input_ids)\n",
    "print(\"Attention Mask:\", attention_mask)\n",
    "\n",
    "# Pass the input to the model\n",
    "output = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "# Get the embeddings\n",
    "embedding = output.last_hidden_state\n",
    "print(\"Embedding shape:\", embedding.shape)\n",
    "print(\"Embedding:\", embedding)\n",
    "\n",
    "de_output=tokenizer.decode(input_ids[0])\n",
    "print(de_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "acb5db89-e48b-4883-8010-cccca2f37596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 4931, 2129, 2024, 2017, 102], 'attention_mask': [1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 10047, 2204, 2129, 2055, 2017, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model=AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "sentence1=\"Hey how are you\"\n",
    "sentence2=\"Im good how about you\"\n",
    "input_ids1=tokenizer(sentence1)\n",
    "input_ids2=tokenizer(sentence2)\n",
    "print(input_ids1)\n",
    "print(input_ids2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "864e4060-982e-4bf6-847c-4ed4fdb1d31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type bert to instantiate a model of type bart. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of BartModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['decoder.embed_positions.weight', 'decoder.embed_tokens.weight', 'decoder.layernorm_embedding.bias', 'decoder.layernorm_embedding.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.fc1.bias', 'decoder.layers.10.fc1.weight', 'decoder.layers.10.fc2.bias', 'decoder.layers.10.fc2.weight', 'decoder.layers.10.final_layer_norm.bias', 'decoder.layers.10.final_layer_norm.weight', 'decoder.layers.10.self_attn.k_proj.bias', 'decoder.layers.10.self_attn.k_proj.weight', 'decoder.layers.10.self_attn.out_proj.bias', 'decoder.layers.10.self_attn.out_proj.weight', 'decoder.layers.10.self_attn.q_proj.bias', 'decoder.layers.10.self_attn.q_proj.weight', 'decoder.layers.10.self_attn.v_proj.bias', 'decoder.layers.10.self_attn.v_proj.weight', 'decoder.layers.10.self_attn_layer_norm.bias', 'decoder.layers.10.self_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.fc1.bias', 'decoder.layers.11.fc1.weight', 'decoder.layers.11.fc2.bias', 'decoder.layers.11.fc2.weight', 'decoder.layers.11.final_layer_norm.bias', 'decoder.layers.11.final_layer_norm.weight', 'decoder.layers.11.self_attn.k_proj.bias', 'decoder.layers.11.self_attn.k_proj.weight', 'decoder.layers.11.self_attn.out_proj.bias', 'decoder.layers.11.self_attn.out_proj.weight', 'decoder.layers.11.self_attn.q_proj.bias', 'decoder.layers.11.self_attn.q_proj.weight', 'decoder.layers.11.self_attn.v_proj.bias', 'decoder.layers.11.self_attn.v_proj.weight', 'decoder.layers.11.self_attn_layer_norm.bias', 'decoder.layers.11.self_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.fc1.bias', 'decoder.layers.6.fc1.weight', 'decoder.layers.6.fc2.bias', 'decoder.layers.6.fc2.weight', 'decoder.layers.6.final_layer_norm.bias', 'decoder.layers.6.final_layer_norm.weight', 'decoder.layers.6.self_attn.k_proj.bias', 'decoder.layers.6.self_attn.k_proj.weight', 'decoder.layers.6.self_attn.out_proj.bias', 'decoder.layers.6.self_attn.out_proj.weight', 'decoder.layers.6.self_attn.q_proj.bias', 'decoder.layers.6.self_attn.q_proj.weight', 'decoder.layers.6.self_attn.v_proj.bias', 'decoder.layers.6.self_attn.v_proj.weight', 'decoder.layers.6.self_attn_layer_norm.bias', 'decoder.layers.6.self_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.fc1.bias', 'decoder.layers.7.fc1.weight', 'decoder.layers.7.fc2.bias', 'decoder.layers.7.fc2.weight', 'decoder.layers.7.final_layer_norm.bias', 'decoder.layers.7.final_layer_norm.weight', 'decoder.layers.7.self_attn.k_proj.bias', 'decoder.layers.7.self_attn.k_proj.weight', 'decoder.layers.7.self_attn.out_proj.bias', 'decoder.layers.7.self_attn.out_proj.weight', 'decoder.layers.7.self_attn.q_proj.bias', 'decoder.layers.7.self_attn.q_proj.weight', 'decoder.layers.7.self_attn.v_proj.bias', 'decoder.layers.7.self_attn.v_proj.weight', 'decoder.layers.7.self_attn_layer_norm.bias', 'decoder.layers.7.self_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.fc1.bias', 'decoder.layers.8.fc1.weight', 'decoder.layers.8.fc2.bias', 'decoder.layers.8.fc2.weight', 'decoder.layers.8.final_layer_norm.bias', 'decoder.layers.8.final_layer_norm.weight', 'decoder.layers.8.self_attn.k_proj.bias', 'decoder.layers.8.self_attn.k_proj.weight', 'decoder.layers.8.self_attn.out_proj.bias', 'decoder.layers.8.self_attn.out_proj.weight', 'decoder.layers.8.self_attn.q_proj.bias', 'decoder.layers.8.self_attn.q_proj.weight', 'decoder.layers.8.self_attn.v_proj.bias', 'decoder.layers.8.self_attn.v_proj.weight', 'decoder.layers.8.self_attn_layer_norm.bias', 'decoder.layers.8.self_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.fc1.bias', 'decoder.layers.9.fc1.weight', 'decoder.layers.9.fc2.bias', 'decoder.layers.9.fc2.weight', 'decoder.layers.9.final_layer_norm.bias', 'decoder.layers.9.final_layer_norm.weight', 'decoder.layers.9.self_attn.k_proj.bias', 'decoder.layers.9.self_attn.k_proj.weight', 'decoder.layers.9.self_attn.out_proj.bias', 'decoder.layers.9.self_attn.out_proj.weight', 'decoder.layers.9.self_attn.q_proj.bias', 'decoder.layers.9.self_attn.q_proj.weight', 'decoder.layers.9.self_attn.v_proj.bias', 'decoder.layers.9.self_attn.v_proj.weight', 'decoder.layers.9.self_attn_layer_norm.bias', 'decoder.layers.9.self_attn_layer_norm.weight', 'encoder.embed_positions.weight', 'encoder.embed_tokens.weight', 'encoder.layernorm_embedding.bias', 'encoder.layernorm_embedding.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.1.fc1.bias', 'encoder.layers.1.fc1.weight', 'encoder.layers.1.fc2.bias', 'encoder.layers.1.fc2.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.10.fc1.bias', 'encoder.layers.10.fc1.weight', 'encoder.layers.10.fc2.bias', 'encoder.layers.10.fc2.weight', 'encoder.layers.10.final_layer_norm.bias', 'encoder.layers.10.final_layer_norm.weight', 'encoder.layers.10.self_attn.k_proj.bias', 'encoder.layers.10.self_attn.k_proj.weight', 'encoder.layers.10.self_attn.out_proj.bias', 'encoder.layers.10.self_attn.out_proj.weight', 'encoder.layers.10.self_attn.q_proj.bias', 'encoder.layers.10.self_attn.q_proj.weight', 'encoder.layers.10.self_attn.v_proj.bias', 'encoder.layers.10.self_attn.v_proj.weight', 'encoder.layers.10.self_attn_layer_norm.bias', 'encoder.layers.10.self_attn_layer_norm.weight', 'encoder.layers.11.fc1.bias', 'encoder.layers.11.fc1.weight', 'encoder.layers.11.fc2.bias', 'encoder.layers.11.fc2.weight', 'encoder.layers.11.final_layer_norm.bias', 'encoder.layers.11.final_layer_norm.weight', 'encoder.layers.11.self_attn.k_proj.bias', 'encoder.layers.11.self_attn.k_proj.weight', 'encoder.layers.11.self_attn.out_proj.bias', 'encoder.layers.11.self_attn.out_proj.weight', 'encoder.layers.11.self_attn.q_proj.bias', 'encoder.layers.11.self_attn.q_proj.weight', 'encoder.layers.11.self_attn.v_proj.bias', 'encoder.layers.11.self_attn.v_proj.weight', 'encoder.layers.11.self_attn_layer_norm.bias', 'encoder.layers.11.self_attn_layer_norm.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.2.fc2.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.3.fc1.weight', 'encoder.layers.3.fc2.bias', 'encoder.layers.3.fc2.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.4.fc1.bias', 'encoder.layers.4.fc1.weight', 'encoder.layers.4.fc2.bias', 'encoder.layers.4.fc2.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.5.fc1.bias', 'encoder.layers.5.fc1.weight', 'encoder.layers.5.fc2.bias', 'encoder.layers.5.fc2.weight', 'encoder.layers.5.final_layer_norm.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.6.fc1.bias', 'encoder.layers.6.fc1.weight', 'encoder.layers.6.fc2.bias', 'encoder.layers.6.fc2.weight', 'encoder.layers.6.final_layer_norm.bias', 'encoder.layers.6.final_layer_norm.weight', 'encoder.layers.6.self_attn.k_proj.bias', 'encoder.layers.6.self_attn.k_proj.weight', 'encoder.layers.6.self_attn.out_proj.bias', 'encoder.layers.6.self_attn.out_proj.weight', 'encoder.layers.6.self_attn.q_proj.bias', 'encoder.layers.6.self_attn.q_proj.weight', 'encoder.layers.6.self_attn.v_proj.bias', 'encoder.layers.6.self_attn.v_proj.weight', 'encoder.layers.6.self_attn_layer_norm.bias', 'encoder.layers.6.self_attn_layer_norm.weight', 'encoder.layers.7.fc1.bias', 'encoder.layers.7.fc1.weight', 'encoder.layers.7.fc2.bias', 'encoder.layers.7.fc2.weight', 'encoder.layers.7.final_layer_norm.bias', 'encoder.layers.7.final_layer_norm.weight', 'encoder.layers.7.self_attn.k_proj.bias', 'encoder.layers.7.self_attn.k_proj.weight', 'encoder.layers.7.self_attn.out_proj.bias', 'encoder.layers.7.self_attn.out_proj.weight', 'encoder.layers.7.self_attn.q_proj.bias', 'encoder.layers.7.self_attn.q_proj.weight', 'encoder.layers.7.self_attn.v_proj.bias', 'encoder.layers.7.self_attn.v_proj.weight', 'encoder.layers.7.self_attn_layer_norm.bias', 'encoder.layers.7.self_attn_layer_norm.weight', 'encoder.layers.8.fc1.bias', 'encoder.layers.8.fc1.weight', 'encoder.layers.8.fc2.bias', 'encoder.layers.8.fc2.weight', 'encoder.layers.8.final_layer_norm.bias', 'encoder.layers.8.final_layer_norm.weight', 'encoder.layers.8.self_attn.k_proj.bias', 'encoder.layers.8.self_attn.k_proj.weight', 'encoder.layers.8.self_attn.out_proj.bias', 'encoder.layers.8.self_attn.out_proj.weight', 'encoder.layers.8.self_attn.q_proj.bias', 'encoder.layers.8.self_attn.q_proj.weight', 'encoder.layers.8.self_attn.v_proj.bias', 'encoder.layers.8.self_attn.v_proj.weight', 'encoder.layers.8.self_attn_layer_norm.bias', 'encoder.layers.8.self_attn_layer_norm.weight', 'encoder.layers.9.fc1.bias', 'encoder.layers.9.fc1.weight', 'encoder.layers.9.fc2.bias', 'encoder.layers.9.fc2.weight', 'encoder.layers.9.final_layer_norm.bias', 'encoder.layers.9.final_layer_norm.weight', 'encoder.layers.9.self_attn.k_proj.bias', 'encoder.layers.9.self_attn.k_proj.weight', 'encoder.layers.9.self_attn.out_proj.bias', 'encoder.layers.9.self_attn.out_proj.weight', 'encoder.layers.9.self_attn.q_proj.bias', 'encoder.layers.9.self_attn.q_proj.weight', 'encoder.layers.9.self_attn.v_proj.bias', 'encoder.layers.9.self_attn.v_proj.weight', 'encoder.layers.9.self_attn_layer_norm.bias', 'encoder.layers.9.self_attn_layer_norm.weight', 'shared.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>token_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>monster</td>\n",
       "      <td>6071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fulham</td>\n",
       "      <td>21703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>intersections</td>\n",
       "      <td>26540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>funds</td>\n",
       "      <td>5029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>##water</td>\n",
       "      <td>5880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30517</th>\n",
       "      <td>balthazar</td>\n",
       "      <td>25021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30518</th>\n",
       "      <td>protested</td>\n",
       "      <td>11456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30519</th>\n",
       "      <td>drifted</td>\n",
       "      <td>10070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30520</th>\n",
       "      <td>maestro</td>\n",
       "      <td>25270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30521</th>\n",
       "      <td>deaths</td>\n",
       "      <td>6677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30522 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               token  token_id\n",
       "0            monster      6071\n",
       "1             fulham     21703\n",
       "2      intersections     26540\n",
       "3              funds      5029\n",
       "4            ##water      5880\n",
       "...              ...       ...\n",
       "30517      balthazar     25021\n",
       "30518      protested     11456\n",
       "30519        drifted     10070\n",
       "30520        maestro     25270\n",
       "30521         deaths      6677\n",
       "\n",
       "[30522 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BartModel, AutoTokenizer\n",
    "import pandas as pd \n",
    "\n",
    "model_name=\"bert-base-uncased\"\n",
    "model=BartModel.from_pretrained (model_name)\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_name) \n",
    "\n",
    "sentence=\"when life gives you lemons, don't make lemonade\"\n",
    "tokens=tokenizer.tokenize(sentence)\n",
    "tokens\n",
    "vocab=tokenizer.vocab\n",
    "vocab_df=pd.DataFrame({\"token\": vocab.keys(),\"token_id\":vocab.values()})\n",
    "vocab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7c20750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[PAD]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[unused0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[unused1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[unused2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[unused3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30517</th>\n",
       "      <td>##．</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30518</th>\n",
       "      <td>##／</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30519</th>\n",
       "      <td>##：</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30520</th>\n",
       "      <td>##？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30521</th>\n",
       "      <td>##～</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30522 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              token\n",
       "token_id           \n",
       "0             [PAD]\n",
       "1         [unused0]\n",
       "2         [unused1]\n",
       "3         [unused2]\n",
       "4         [unused3]\n",
       "...             ...\n",
       "30517           ##．\n",
       "30518           ##／\n",
       "30519           ##：\n",
       "30520           ##？\n",
       "30521           ##～\n",
       "\n",
       "[30522 rows x 1 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_df =vocab_df.sort_values(by='token_id').set_index('token_id')\n",
    "vocab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e65252a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'values' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m token_ids\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mencode(sentence)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mlen\u001b[39m(values)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mlen\u001b[39m(token_ids)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'values' is not defined"
     ]
    }
   ],
   "source": [
    "token_ids=tokenizer.encode(sentence)\n",
    "len(values)\n",
    "len(token_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f29763",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
